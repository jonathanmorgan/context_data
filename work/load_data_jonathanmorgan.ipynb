{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load_data_jonathanmorgan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Data-file\" data-toc-modified-id=\"Data-file-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data file</a></span></li><li><span><a href=\"#Create-Articles\" data-toc-modified-id=\"Create-Articles-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create Articles</a></span></li><li><span><a href=\"#Create-DataSets\" data-toc-modified-id=\"Create-DataSets-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create DataSets</a></span></li><li><span><a href=\"#Create-Citations\" data-toc-modified-id=\"Create-Citations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Create Citations</a></span></li><li><span><a href=\"#Tag-citations\" data-toc-modified-id=\"Tag-citations-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Tag citations</a></span></li><li><span><a href=\"#Update-Articles\" data-toc-modified-id=\"Update-Articles-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Update Articles</a></span><ul class=\"toc-item\"><li><span><a href=\"#Add-PDF-file-name-to-Article\" data-toc-modified-id=\"Add-PDF-file-name-to-Article-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Add PDF file name to Article</a></span></li><li><span><a href=\"#Update-Article_Text-for-publications\" data-toc-modified-id=\"Update-Article_Text-for-publications-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Update Article_Text for publications</a></span></li><li><span><a href=\"#Update-Article_Text-content_type\" data-toc-modified-id=\"Update-Article_Text-content_type-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Update Article_Text content_type</a></span></li></ul></li><li><span><a href=\"#Set-dataset-family_identifier\" data-toc-modified-id=\"Set-dataset-family_identifier-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Set dataset family_identifier</a></span></li><li><span><a href=\"#Output\" data-toc-modified-id=\"Output-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Output</a></span></li><li><span><a href=\"#Contest-Data\" data-toc-modified-id=\"Contest-Data-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Contest Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Output-Article-data\" data-toc-modified-id=\"Output-Article-data-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Output Article data</a></span></li><li><span><a href=\"#Output-DataSet-data\" data-toc-modified-id=\"Output-DataSet-data-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Output DataSet data</a></span></li><li><span><a href=\"#Output-DataSetCitation-data\" data-toc-modified-id=\"Output-DataSetCitation-data-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Output DataSetCitation data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run django_init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_folder = \"/home/ubuntu/work/data/rich_context_test_data\"\n",
    "\n",
    "# CSV file name\n",
    "csv_file_name = \"jstor_top_100_icpsr_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data file\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for accessing rows in CSV\n",
    "column_name_to_index_map = {}\n",
    "column_name_to_index_map[ \"dataDate\" ] = 0\n",
    "column_name_to_index_map[ \"dataId\" ] = 1\n",
    "column_name_to_index_map[ \"dataTitle\" ] = 2\n",
    "column_name_to_index_map[ \"pubDate\" ] = 3\n",
    "column_name_to_index_map[ \"pubId\" ] = 4\n",
    "column_name_to_index_map[ \"pubIdSchema\" ] = 5\n",
    "column_name_to_index_map[ \"pubObjectSubType\" ] = 6\n",
    "column_name_to_index_map[ \"pubObjectType\" ] = 7\n",
    "column_name_to_index_map[ \"pubTitle\" ] = 8\n",
    "column_name_to_index_map[ \"recordId\" ] = 9\n",
    "column_name_to_index_map[ \"pred\" ] = 10\n",
    "column_name_to_index_map[ \"jstor_filename\" ] = 11\n",
    "column_name_to_index_map[ \"jstor_filename_txt\" ] = 12\n",
    "\n",
    "# constants-ish that contain index for column name.\n",
    "CSV_INDEX_DATA_DATE = column_name_to_index_map[ \"dataDate\" ]\n",
    "CSV_INDEX_DATA_ID = column_name_to_index_map[ \"dataId\" ]\n",
    "CSV_INDEX_DATA_TITLE = column_name_to_index_map[ \"dataTitle\" ]\n",
    "CSV_INDEX_PUB_DATE = column_name_to_index_map[ \"pubDate\" ]\n",
    "CSV_INDEX_PUB_ID = column_name_to_index_map[ \"pubId\" ]\n",
    "CSV_INDEX_PUB_ID_SCHEMA = column_name_to_index_map[ \"pubIdSchema\" ]\n",
    "CSV_INDEX_PUB_OBJECT_SUB_TYPE = column_name_to_index_map[ \"pubObjectSubType\" ]\n",
    "CSV_INDEX_PUB_OBJECT_TYPE = column_name_to_index_map[ \"pubObjectType\" ]\n",
    "CSV_INDEX_PUB_TITLE = column_name_to_index_map[ \"pubTitle\" ]\n",
    "CSV_INDEX_RECORD_ID = column_name_to_index_map[ \"recordId\" ]\n",
    "CSV_INDEX_PRED = column_name_to_index_map[ \"pred\" ]\n",
    "CSV_INDEX_JSTOR_FILE_NAME = column_name_to_index_map[ \"jstor_filename\" ]\n",
    "CSV_INDEX_JSTOR_FILE_NAME_TXT = column_name_to_index_map[ \"jstor_filename_txt\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "csv_file = None\n",
    "data_import_reader = None\n",
    "row = None\n",
    "\n",
    "# declare variables - processing control\n",
    "last_pub_id = None\n",
    "\n",
    "# Open CSV file for reading\n",
    "with open( csv_file_name ) as csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.reader( csv_file )\n",
    "    \n",
    "    # loop over rows and just output each, for now.\n",
    "    last_pub_id = -1\n",
    "    for row in data_import_reader:\n",
    "        \n",
    "        print( row )\n",
    "        \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Articles\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, we will loop over the rows in the CSV and create Article and Article_Text for each publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sourcenet.models import Article\n",
    "from sourcenet.models import Article_Notes\n",
    "from sourcenet.models import Article_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "csv_file = None\n",
    "data_import_reader = None\n",
    "row = None\n",
    "pub_id = None\n",
    "pub_date = None\n",
    "pub_id_schema = None\n",
    "pub_object_sub_type = None\n",
    "pub_object_type = None\n",
    "pub_title = None\n",
    "pred = None\n",
    "pdf_file_name = None\n",
    "text_file_name = None\n",
    "\n",
    "# declare variables - processing control\n",
    "last_pub_id = None\n",
    "article_counter = -1\n",
    "article_qs = None\n",
    "article_count = -1\n",
    "current_article = None\n",
    "pub_date_string = None\n",
    "\n",
    "# declare variables - Article_Notes\n",
    "current_article_notes = None\n",
    "\n",
    "# declare variables - Article_Text\n",
    "current_article_text = None\n",
    "body_text_file_path = None\n",
    "body_text_file = None\n",
    "body_text = None\n",
    "\n",
    "# Open CSV file for reading\n",
    "article_counter = 0\n",
    "with open( csv_file_name ) as csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.DictReader( csv_file )\n",
    "    \n",
    "    # loop over rows and just output each, for now.\n",
    "    last_pub_id = -1\n",
    "    for row_dict in data_import_reader:\n",
    "        \n",
    "        # get pub_id\n",
    "        pub_id = row_dict.get( \"pubId\", -1 )\n",
    "        \n",
    "        # see if already in database.\n",
    "        article_qs = Article.objects.filter( unique_identifier = pub_id )\n",
    "        article_count = article_qs.count()\n",
    "        if ( article_count <= 0 ):\n",
    "        \n",
    "            # not in database - is last different from current?\n",
    "            if ( ( pub_id != None ) and ( pub_id != \"\" ) and( pub_id != last_pub_id ) ):\n",
    "\n",
    "                article_counter += 1\n",
    "                \n",
    "                # changed.  Get publication data\n",
    "                pub_id_schema = row_dict.get( \"pubIdSchema\", None )\n",
    "                pub_date_string = row_dict.get( \"pubDate\", None )\n",
    "                pub_date = datetime.datetime.strptime( pub_date_string, \"%Y-%m-%d\" ).date()\n",
    "                pub_object_sub_type = row_dict.get( \"pubObjectSubType\", None )\n",
    "                pub_object_type = row_dict.get( \"pubObjectType\", None )\n",
    "                pub_title = row_dict.get( \"pubTitle\", None )\n",
    "                text_file_name = row_dict.get( \"jstor_filename_txt\", None )\n",
    "                pdf_file_name = row_dict.get( \"jstor_filename\", None )\n",
    "\n",
    "                # create Article\n",
    "                current_article = Article()\n",
    "                current_article.headline = pub_title\n",
    "                current_article.unique_identifier = pub_id\n",
    "                current_article.permalink = pub_id\n",
    "                current_article.pub_date = pub_date\n",
    "                current_article.file_path = pdf_file_name;\n",
    "                current_article.save()\n",
    "                print( \"==> Processed article {}: {} ( row: {} )\".format( article_counter, current_article, row_dict ) )\n",
    "\n",
    "                # Store full row in Article_Note\n",
    "                current_article_notes = Article_Notes()\n",
    "                current_article_notes.article = current_article\n",
    "                current_article_notes.type = \"row_dict\"\n",
    "                current_article_notes.content = str( row_dict )\n",
    "                current_article_notes.save()\n",
    "\n",
    "                # load text.\n",
    "                body_text_file_path = \"txt/{}\".format( text_file_name )\n",
    "                with open( body_text_file_path, encoding = \"windows-1252\" ) as body_text_file:\n",
    "\n",
    "                    # read in body text\n",
    "                    body_text = body_text_file.read()\n",
    "\n",
    "                    # got anything?\n",
    "                    if ( ( body_text is not None ) and ( body_text != \"\" ) ):\n",
    "\n",
    "                        # yes.  create Article_Text, and add it.\n",
    "                        #print( \"Got body text\" )\n",
    "                        current_article_text = Article_Text()\n",
    "                        current_article_text.article = current_article\n",
    "                        current_article_text.content = body_text\n",
    "                        current_article_text.content_type = Article_Text.CONTENT_TYPE_TEXT\n",
    "                        current_article_text.do_clean_on_save = False\n",
    "                        current_article_text.save()\n",
    "\n",
    "                    #-- END check to see if body text. --#\n",
    "\n",
    "                #-- END with open( body_text_file_path ) as body_text_file:\n",
    "\n",
    "            #-- END check to see if new ID. --#\n",
    "        \n",
    "        #-- END check to see if already in database --#\n",
    "        \n",
    "        last_pub_id = pub_id\n",
    "            \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#\n",
    "\n",
    "print( \"PROCESSED {} ARTICLES\".format( article_counter ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataSets\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Next, we loop over the rows in the CSV and create a DataSet for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from context_data.models import DataSet\n",
    "from context_data.models import DataSetIdentifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "csv_file = None\n",
    "data_import_reader = None\n",
    "row = None\n",
    "data_id = None\n",
    "data_date_string = None\n",
    "data_date = None\n",
    "data_title = None\n",
    "\n",
    "# declare variables - processing control\n",
    "data_set_counter = -1\n",
    "last_pub_id = None\n",
    "data_set_qs = None\n",
    "data_set_count = -1\n",
    "current_data_set = None\n",
    "data_date_string = None\n",
    "current_data_set_identifier = None\n",
    "\n",
    "# Open CSV file for reading\n",
    "data_set_counter = 0\n",
    "with open( csv_file_name ) as csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.DictReader( csv_file )\n",
    "    \n",
    "    # loop over rows.\n",
    "    for row_dict in data_import_reader:\n",
    "        \n",
    "        # get data_id\n",
    "        data_id = row_dict.get( \"dataId\", -1 )\n",
    "        \n",
    "        # see if already in database.\n",
    "        data_set_qs = DataSet.objects.filter( unique_identifier = data_id )\n",
    "        data_set_count = data_set_qs.count()\n",
    "        if ( data_set_count <= 0 ):\n",
    "        \n",
    "            data_set_counter += 1\n",
    "            \n",
    "            # not in database - get data set data\n",
    "            data_date_string = row_dict.get( \"dataDate\", None )\n",
    "            data_date = datetime.datetime.strptime( data_date_string, \"%Y-%m-%d\" ).date()\n",
    "            data_title = row_dict.get( \"dataTitle\", None )\n",
    "\n",
    "            # create DataSet\n",
    "            current_data_set = DataSet()\n",
    "            current_data_set.name = data_title\n",
    "            current_data_set.title = data_title\n",
    "            current_data_set.unique_identifier = data_id\n",
    "            current_data_set.data = data_date\n",
    "            current_data_set.additional_keywords = \"ICPSR\"\n",
    "            current_data_set.save()\n",
    "            print( \"==> Processed DataSet {}: {} ( row: {} )\".format( data_set_counter, current_data_set, row_dict ) )\n",
    "\n",
    "            # Creat DataSetIdentifier for ICPSR ID\n",
    "            current_data_set_identifier = DataSetIdentifier()\n",
    "            current_data_set_identifier.data_set = current_data_set\n",
    "            current_data_set_identifier.name = \"ICPSR data ID (dataId)\"\n",
    "            current_data_set_identifier.identifier = data_id\n",
    "            current_data_set_identifier.source = \"ICPSR, University of Michigan\"\n",
    "            current_data_set_identifier.save()\n",
    "            print( \"----> Processed DataSetIdentifier: {} )\".format( current_data_set_identifier ) )\n",
    "        \n",
    "        #-- END check to see if already in database --#\n",
    "            \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#\n",
    "\n",
    "print( \"PROCESSED {} DATA SETS\".format( data_set_counter ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Citations\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For each row in spreadsheet, get pub ID and data ID, look up Article and DataSet for each, then make DataSetCitation record joining the two together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sourcenet.models import Article\n",
    "from context_data.models import DataSet\n",
    "from context_data.models import DataSetCitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "csv_file = None\n",
    "data_import_reader = None\n",
    "row = None\n",
    "pub_id = None\n",
    "data_id = None\n",
    "citation_type = \"\"\n",
    "\n",
    "# declare variables - processing control\n",
    "citation_counter = -1\n",
    "article_qs = None\n",
    "article_count = -1\n",
    "current_article = None\n",
    "data_set_qs = None\n",
    "data_set_count = -1\n",
    "current_data_set = None\n",
    "citation_qs = None\n",
    "citation_count = -1\n",
    "current_citation = None\n",
    "\n",
    "# these are all citations of data sets used in analysis.\n",
    "citation_type = DataSetCitation.CITATION_TYPE_ANALYSIS\n",
    "\n",
    "# Open CSV file for reading\n",
    "citation_counter = 0\n",
    "with open( csv_file_name ) as csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.DictReader( csv_file )\n",
    "    \n",
    "    # loop over rows.\n",
    "    for row_dict in data_import_reader:\n",
    "        \n",
    "        # get article\n",
    "        pub_id = row_dict.get( \"pubId\", -1 )\n",
    "        current_article = Article.objects.get( unique_identifier = pub_id )\n",
    "        \n",
    "        # get data_set\n",
    "        data_id = row_dict.get( \"dataId\", -1 )\n",
    "        current_data_set = DataSet.objects.get( unique_identifier = data_id )\n",
    "        \n",
    "        # see if already in database.\n",
    "        citation_qs = DataSetCitation.objects.filter( article = current_article )\n",
    "        citation_qs = citation_qs.filter( data_set = current_data_set )\n",
    "        citation_count = citation_qs.count()\n",
    "        if ( citation_count <= 0 ):\n",
    "        \n",
    "            citation_counter += 1\n",
    "            \n",
    "            # not in database - create DataSetCitation\n",
    "            current_citation = DataSetCitation()\n",
    "            current_citation.article = current_article\n",
    "            current_citation.data_set = current_data_set\n",
    "            current_citation.type = citation_type\n",
    "            current_citation.save()\n",
    "            print( \"==> Processed DataSetCitation {}: {} ( row: {} )\".format( citation_counter, current_citation, row_dict ) )\n",
    "        \n",
    "        #-- END check to see if already in database --#\n",
    "            \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#\n",
    "\n",
    "print( \"PROCESSED {} DATA SET CITATIONS\".format( citation_counter ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag citations\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Tag 100 citations with \"icpsr_coding_test\" so we see them in coding app (this is a lot - might need to improve the test list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "django-taggit documentation: https://github.com/alex/django-taggit\n",
    "\n",
    "Adding tags to a model:\n",
    "\n",
    "    from django.db import models\n",
    "    \n",
    "    from taggit.managers import TaggableManager\n",
    "    \n",
    "    class Food(models.Model):\n",
    "        # ... fields here\n",
    "    \n",
    "        tags = TaggableManager()\n",
    "\n",
    "Interacting with a model that has tags:\n",
    "\n",
    "    >>> apple = Food.objects.create(name=\"apple\")\n",
    "    >>> apple.tags.add(\"red\", \"green\", \"delicious\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: green>, <Tag: delicious>]\n",
    "    >>> apple.tags.remove(\"green\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: delicious>]\n",
    "    >>> Food.objects.filter(tags__name__in=[\"red\"])\n",
    "    [<Food: apple>, <Food: cherry>]\n",
    "    \n",
    "    # include only those with certain tags.\n",
    "    #tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "    tags_in_list = [ \"grp_month\", ]\n",
    "    if ( len( tags_in_list ) > 0 ):\n",
    "    \n",
    "        # filter\n",
    "        print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "        grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "        \n",
    "    #-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "'''\n",
    "\n",
    "# imports\n",
    "from context_data.models import DataSetCitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "citation_qs = None\n",
    "citation_count = -1\n",
    "current_citation = None\n",
    "tag_value = \"\"\n",
    "\n",
    "# set tag value\n",
    "tag_value = \"icpsr_coding_test\"\n",
    "\n",
    "# create query set.\n",
    "citation_qs = DataSetCitation.objects.all()\n",
    "\n",
    "# don't get any that already have this tag.\n",
    "citation_qs = citation_qs.exclude( tags__name__in = [ tag_value ] )\n",
    "\n",
    "# limit to 100 for now\n",
    "citation_qs = citation_qs[ 0 : 100 ]\n",
    "\n",
    "# could also filter...\n",
    "\n",
    "# first, just make sure that worked.\n",
    "citation_count = citation_qs.count()\n",
    "\n",
    "# Check count of citations retrieved.\n",
    "print( \"Got \" + str( citation_count ) + \" citations to tag with \\\"\" + tag_value + \"\\\".\" )\n",
    "\n",
    "# loop over citations.\n",
    "for current_citation in citation_qs:\n",
    "\n",
    "    # add tag.\n",
    "    current_citation.tags.add( tag_value )\n",
    "    \n",
    "    # output the tags.\n",
    "    print( \"- Tags for citation \" + str( current_citation.id ) + \" : \" + str( current_citation.tags.all() ) )\n",
    "    \n",
    "#-- END loop over citations --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Articles\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, we will loop over the rows in the CSV and create Article and Article_Text for each publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add PDF file name to Article\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sourcenet.models import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "csv_file = None\n",
    "data_import_reader = None\n",
    "row = None\n",
    "pub_id = None\n",
    "pdf_file_name = None\n",
    "\n",
    "# declare variables - processing control\n",
    "last_pub_id = None\n",
    "article_counter = -1\n",
    "article_qs = None\n",
    "article_count = -1\n",
    "current_article = None\n",
    "\n",
    "# Open CSV file for reading\n",
    "article_counter = 0\n",
    "with open( csv_file_name ) as csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.DictReader( csv_file )\n",
    "    \n",
    "    # loop over rows and just output each, for now.\n",
    "    last_pub_id = -1\n",
    "    for row_dict in data_import_reader:\n",
    "        \n",
    "        # get pub_id\n",
    "        pub_id = row_dict.get( \"pubId\", -1 )\n",
    "        \n",
    "        # see if already in database.\n",
    "        article_qs = Article.objects.filter( unique_identifier = pub_id )\n",
    "        article_count = article_qs.count()\n",
    "        if ( article_count == 1 ):\n",
    "        \n",
    "            # yes.  Load  in database - is last different from current?\n",
    "            if ( ( pub_id != None ) and ( pub_id != \"\" ) and( pub_id != last_pub_id ) ):\n",
    "\n",
    "                article_counter += 1\n",
    "                \n",
    "                # load the article.\n",
    "                current_article = article_qs.get()\n",
    "                \n",
    "                # store pdf file \n",
    "                pdf_file_name = row_dict.get( \"jstor_filename\", None )\n",
    "                current_article.file_path = pdf_file_name;\n",
    "                current_article.save()\n",
    "\n",
    "            #-- END check to see if new ID. --#\n",
    "        \n",
    "        #-- END check to see if already in database --#\n",
    "        \n",
    "        last_pub_id = pub_id\n",
    "            \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#\n",
    "\n",
    "print( \"PROCESSED {} ARTICLES\".format( article_counter ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Article_Text for publications\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, we will loop over the rows in the CSV and update Article_Text from files for each publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sourcenet.models import Article\n",
    "from sourcenet.models import Article_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "csv_file = None\n",
    "data_import_reader = None\n",
    "row = None\n",
    "pub_id = None\n",
    "text_file_name = None\n",
    "\n",
    "# declare variables - processing control\n",
    "last_pub_id = None\n",
    "article_counter = -1\n",
    "article_qs = None\n",
    "article_count = -1\n",
    "current_article = None\n",
    "\n",
    "# declare variables - Article_Notes\n",
    "current_article_notes = None\n",
    "\n",
    "# declare variables - Article_Text\n",
    "current_article_text = None\n",
    "body_text_file_path = None\n",
    "body_text_file = None\n",
    "body_text = None\n",
    "\n",
    "# Open CSV file for reading\n",
    "article_counter = 0\n",
    "with open( csv_file_name ) as csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.DictReader( csv_file )\n",
    "    \n",
    "    # loop over rows and just output each, for now.\n",
    "    last_pub_id = -1\n",
    "    for row_dict in data_import_reader:\n",
    "        \n",
    "        # get pub_id\n",
    "        pub_id = row_dict.get( \"pubId\", -1 )\n",
    "        \n",
    "        # see if already in database.\n",
    "        article_qs = Article.objects.filter( unique_identifier = pub_id )\n",
    "        article_count = article_qs.count()\n",
    "        if ( article_count == 1 ):\n",
    "        \n",
    "            # yes.  Is last different from current?\n",
    "            if ( ( pub_id != None ) and ( pub_id != \"\" ) and( pub_id != last_pub_id ) ):\n",
    "\n",
    "                # yes - process another one.\n",
    "                article_counter += 1\n",
    "                \n",
    "                # Get file information.\n",
    "                text_file_name = row_dict.get( \"jstor_filename_txt\", None )\n",
    "                pdf_file_name = row_dict.get( \"jstor_filename\", None )\n",
    "\n",
    "                # retrieve Article_Text\n",
    "                current_article = article_qs.get()\n",
    "                current_article_text = current_article.article_text_set.get()\n",
    "\n",
    "                # load text.\n",
    "                body_text_file_path = \"txt/{}\".format( text_file_name )\n",
    "                with open( body_text_file_path, encoding = \"windows-1252\" ) as body_text_file:\n",
    "\n",
    "                    # read in body text\n",
    "                    body_text = body_text_file.read()\n",
    "\n",
    "                    # got anything?\n",
    "                    if ( ( body_text is not None ) and ( body_text != \"\" ) ):\n",
    "\n",
    "                        # yes.  create Article_Text, and add it.\n",
    "                        #print( \"Got body text\" )\n",
    "                        current_article_text.content = body_text\n",
    "                        current_article_text.do_clean_on_save = False\n",
    "                        current_article_text.save()\n",
    "                        \n",
    "                        print( \"Updated Article_Text {}: {}\".format( article_counter, current_article_text ) )\n",
    "\n",
    "                    #-- END check to see if body text. --#\n",
    "\n",
    "                #-- END with open( body_text_file_path ) as body_text_file:\n",
    "\n",
    "            #-- END check to see if new ID. --#\n",
    "        \n",
    "        #-- END check to see if already in database --#\n",
    "        \n",
    "        last_pub_id = pub_id\n",
    "            \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#\n",
    "\n",
    "print( \"PROCESSED {} ARTICLES\".format( article_counter ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Article_Text content_type\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, we will loop over the rows in the CSV and update Article_Text from files for each publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sourcenet.models import Article\n",
    "from sourcenet.models import Article_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "csv_file = None\n",
    "data_import_reader = None\n",
    "row = None\n",
    "pub_id = None\n",
    "text_file_name = None\n",
    "\n",
    "# declare variables - processing control\n",
    "last_pub_id = None\n",
    "article_counter = -1\n",
    "article_qs = None\n",
    "article_count = -1\n",
    "current_article = None\n",
    "\n",
    "# declare variables - Article_Notes\n",
    "current_article_notes = None\n",
    "\n",
    "# declare variables - Article_Text\n",
    "current_article_text = None\n",
    "body_text_file_path = None\n",
    "body_text_file = None\n",
    "body_text = None\n",
    "\n",
    "# Open CSV file for reading\n",
    "article_counter = 0\n",
    "with open( csv_file_name ) as csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.DictReader( csv_file )\n",
    "    \n",
    "    # loop over rows and just output each, for now.\n",
    "    last_pub_id = -1\n",
    "    for row_dict in data_import_reader:\n",
    "        \n",
    "        # get pub_id\n",
    "        pub_id = row_dict.get( \"pubId\", -1 )\n",
    "        \n",
    "        # see if already in database.\n",
    "        article_qs = Article.objects.filter( unique_identifier = pub_id )\n",
    "        article_count = article_qs.count()\n",
    "        if ( article_count == 1 ):\n",
    "        \n",
    "            # yes.  Is last different from current?\n",
    "            if ( ( pub_id != None ) and ( pub_id != \"\" ) and( pub_id != last_pub_id ) ):\n",
    "\n",
    "                # yes - process another one.\n",
    "                article_counter += 1\n",
    "                \n",
    "                # retrieve Article_Text\n",
    "                current_article = article_qs.get()\n",
    "                current_article_text = current_article.article_text_set.get()\n",
    "\n",
    "                # yes.  create Article_Text, and add it.\n",
    "                #print( \"Got body text\" )\n",
    "                current_article_text.content_type = Article_Text.CONTENT_TYPE_TEXT\n",
    "                current_article_text.do_clean_on_save = False\n",
    "                current_article_text.save()\n",
    "                        \n",
    "                print( \"Updated Article_Text {}: {}\".format( article_counter, current_article_text ) )\n",
    "\n",
    "                #-- END with open( body_text_file_path ) as body_text_file:\n",
    "\n",
    "            #-- END check to see if new ID. --#\n",
    "        \n",
    "        #-- END check to see if already in database --#\n",
    "        \n",
    "        last_pub_id = pub_id\n",
    "            \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#\n",
    "\n",
    "print( \"PROCESSED {} ARTICLES\".format( article_counter ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set dataset family_identifier\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Read in CSV of data sets where each row has:\n",
    "\n",
    "- data set unique identifier\n",
    "- an assigned family identifer\n",
    "- and the title of the data set.\n",
    "\n",
    "For each, get the data set with the unique identifier, and if found, add the family identifier then save()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from context_data.models import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "family_id_csv_file_name = \"\"\n",
    "family_id_csv_file = None\n",
    "data_import_reader = None\n",
    "row_dict = None\n",
    "data_set_review = None\n",
    "data_set_unique_id = None\n",
    "data_set_family_id = None\n",
    "data_set_title = None\n",
    "\n",
    "# declare variables - processing control\n",
    "dataset_counter = -1\n",
    "error_counter = -1\n",
    "data_set_qs = None\n",
    "data_set_count = -1\n",
    "current_data_set = None\n",
    "\n",
    "# Open CSV file for reading\n",
    "family_id_csv_file_name = \"rich_context_icpsr_dataset_families.csv\"\n",
    "dataset_counter = 0\n",
    "error_counter = 0\n",
    "with open( family_id_csv_file_name ) as family_id_csv_file:\n",
    "    \n",
    "    # create CSV reader so we can loop.\n",
    "    data_import_reader = csv.DictReader( family_id_csv_file )\n",
    "    \n",
    "    # loop over rows.\n",
    "    for row_dict in data_import_reader:\n",
    "        \n",
    "        # get data set info\n",
    "        data_set_review = row_dict.get( \"review\", None )\n",
    "        data_set_unique_id = row_dict.get( \"dataId\", None )\n",
    "        data_set_family_id = row_dict.get( \"family_id\", None )\n",
    "        data_set_title = row_dict.get( \"dataTitle\", None )\n",
    "        \n",
    "        # got a unique and family identifiers?\n",
    "        if ( ( data_set_unique_id is not None ) and ( data_set_unique_id != \"\" ) ):\n",
    "            \n",
    "            if ( ( data_set_family_id is not None ) and ( data_set_family_id != \"\" ) ):\n",
    "        \n",
    "                # look in database for unique_identifier.\n",
    "                data_set_qs = DataSet.objects.filter( unique_identifier = data_set_unique_id )\n",
    "                data_set_count = data_set_qs.count()\n",
    "                if ( data_set_count == 1 ):\n",
    "        \n",
    "                    # Found.  Get.\n",
    "                    current_data_set = data_set_qs.get()\n",
    "\n",
    "                    # set family_identifier\n",
    "                    current_data_set.family_identifier = data_set_family_id\n",
    "                    \n",
    "                    # save\n",
    "                    current_data_set.save()\n",
    "                    \n",
    "                    dataset_counter += 1\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    print( \"No data set found for identifier {}\".format( data_set_unique_id ) )\n",
    "                    error_counter += 1\n",
    "                    \n",
    "                #-- END check to see if only one record. --#\n",
    "\n",
    "            else:\n",
    "                \n",
    "                print( \"No family ID for data set {}.  Moving on.\".format( data_set_unique_id ) )\n",
    "                error_counter += 1\n",
    "                \n",
    "            #-- END check to see if family identifier. --#\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print( \"No unique identifier for row.  Moving on (and check the column names you are using to retrieve).\" )\n",
    "            error_counter += 1\n",
    "            \n",
    "        #-- END check to see if already in database --#\n",
    "        \n",
    "    #-- END loop over CSV file --#\n",
    "    \n",
    "#-- END with open( csv_file_name ) as csv_file: --#\n",
    "\n",
    "print( \"PROCESSED {} DATA SETS, error count: {}\".format( dataset_counter, error_counter ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "A quick program to output data saved so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sourcenet.models import Article\n",
    "from sourcenet.models import Article_Data\n",
    "from context_data.models import DataSet\n",
    "from context_data.models import DataSetCitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "citation_qs = None\n",
    "citation = None\n",
    "article = None\n",
    "data_set = None\n",
    "article_data_qs = None\n",
    "data_list = None\n",
    "article_data = None\n",
    "coder = None\n",
    "mention_qs = None\n",
    "mention_count = None\n",
    "mention_list = None\n",
    "mention = None\n",
    "mention_value = None\n",
    "mention_text = None\n",
    "item = None\n",
    "\n",
    "# get all Citations.\n",
    "citation_qs = DataSetCitation.objects.all().order_by( \"article\", \"data_set\" )\n",
    "\n",
    "# loop over articles to see which have \n",
    "for citation in citation_qs:\n",
    "    \n",
    "    # get article\n",
    "    article = citation.article\n",
    "    \n",
    "    # get data set\n",
    "    data_set = citation.data_set\n",
    "    \n",
    "    # get Article_Data for article\n",
    "    article_data_qs = Article_Data.objects.filter( article = article )\n",
    "    \n",
    "    # get mentions.\n",
    "    data_list = []\n",
    "    for article_data in article_data_qs:\n",
    "        \n",
    "        # get coder\n",
    "        coder = article_data.coder\n",
    "        \n",
    "        # get mentions direct\n",
    "        mention_qs = article_data.datasetcitationdata_set.datasetmention_set.filter( data_set_citation__data_set = data_set )\n",
    "        mention_count = mention_qs.count()\n",
    "        if ( mention_count > 0 ):\n",
    "        \n",
    "            # loop on mentions for user, making a list of values.\n",
    "            mention_list = []\n",
    "            for mention in mention_qs:\n",
    "\n",
    "                mention_value = mention.value\n",
    "                mention_list.append( mention_value )\n",
    "\n",
    "            #-- END loop over mentions. --#\n",
    "\n",
    "            # output mentions\n",
    "            mention_text = \"-------- coder {}: {}\".format( coder, mention_list )\n",
    "            data_list.append( mention_text )\n",
    "            \n",
    "        #-- END check to see if any mentions --#\n",
    "       \n",
    "    #-- END loop over Article_Data --#\n",
    "    \n",
    "    if ( len( data_list ) > 0 ):\n",
    "        \n",
    "        print()\n",
    "        print( \"==> Citation:\" )\n",
    "        print( \"----> Article: {}\".format( article ) )\n",
    "        print( \"----> DataSet: {}\".format( data_set ) )\n",
    "        for item in data_list:\n",
    "            \n",
    "            print( item )\n",
    "\n",
    "        #-- END loop over data --#\n",
    "        \n",
    "    #-- END check to see if data for this citation. --#\n",
    "    \n",
    "#-- END loop over citations --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contest Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Create the contest data:\n",
    "\n",
    "- JSON files\n",
    "- article:\n",
    "\n",
    "    - JSON of metadata\n",
    "    - dump full text for each to <article_id>.txt\n",
    "    - make shel script file that contains a cp for each PDF from webroot's folder to a place where we keep the competition data (shared - in web root with 777?).\n",
    "\n",
    "- dataset:\n",
    "\n",
    "    - JSON of metadata plus mention list across all articles.\n",
    "\n",
    "- citations:\n",
    "\n",
    "    - JSON with citation ID, article ID, data set ID, and then list of mentions in the article.\n",
    "\n",
    "- will make a tar file of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Information we need for data creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to get PDF files.\n",
    "context_home_directory = \"/home/context\"\n",
    "pdf_source_directory_path = \"/var/www/html/pdf\"\n",
    "data_output_directory = \"{}/data\".format( context_home_directory )\n",
    "pdf_output_directory_path = \"{}/files/pdf\".format( data_output_directory )\n",
    "text_output_directory_path = \"{}/files/text\".format( data_output_directory )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Article data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, need to make a file of article data - could make CSV or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from python_utilities.json.json_helper import JSONHelper\n",
    "from sourcenet.models import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article.objects.all().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article.objects.filter( article_data__datasetcitationdata__id__gte = 0 ).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article.objects.filter( datasetcitation__datasetcitationdata__id__gte = 0 ).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article.objects.filter( article_data__id__gte = 0 ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "article_qs = None\n",
    "article_count = -1\n",
    "article_counter = -1\n",
    "article = None\n",
    "article_json_root = []\n",
    "article_json = {}\n",
    "\n",
    "# declare variables - output shell script for renaming PDF files.\n",
    "pdf_script_file_path = \"{}/build_pdf_files.sh\".format( data_output_directory )\n",
    "pdf_script_file = None\n",
    "\n",
    "# declare variables - article information.\n",
    "article_id = None\n",
    "article_unique_identifier = None\n",
    "article_headline = None\n",
    "article_pub_date = None\n",
    "article_file_path = None\n",
    "article_pdf_file_name = None\n",
    "article_text_file_name = None\n",
    "\n",
    "# declare variables - output text files.\n",
    "text_file_output_folder_path = text_output_directory_path\n",
    "article_text = None\n",
    "article_text_string = None\n",
    "text_file_path = None\n",
    "text_file = None\n",
    "\n",
    "# declare variables - output JSON file.\n",
    "json_file_path = None\n",
    "json_file = None\n",
    "\n",
    "# get only Articles who have an associated Article_Data and DataSetCitationData\n",
    "article_qs = Article.objects.filter( article_data__datasetcitationdata__id__gte = 0 ).distinct().order_by( \"id\" )\n",
    "article_count = article_qs.count()\n",
    "print( \"articles to process: {}\".format( article_count ) )\n",
    "\n",
    "# init PDF script file.\n",
    "with open( pdf_script_file_path, \"w\", encoding = \"utf-8\" ) as pdf_script_file:\n",
    "    \n",
    "    # start with shebang line.\n",
    "    pdf_script_file.write( '#!/bin/bash\\n' )\n",
    "\n",
    "    # loop over articles\n",
    "    article_counter = 0\n",
    "    for article in article_qs:\n",
    "\n",
    "        # increment counter\n",
    "        article_counter += 1\n",
    "        \n",
    "        # get article information we'll use.\n",
    "        article_id = article.id\n",
    "        article_unique_identifier = article.unique_identifier\n",
    "        article_headline = article.headline\n",
    "        article_pub_date = article.pub_date\n",
    "        article_file_path = article.file_path\n",
    "\n",
    "        # generate information related to files.\n",
    "        article_pdf_file_name = \"{}.pdf\".format( article_id )\n",
    "        article_text_file_name = \"{}.txt\".format( article_id )\n",
    "\n",
    "        # build article JSON dictionary.\n",
    "        article_json = {}\n",
    "        article_json[ \"id\" ] = article_id\n",
    "        article_json[ \"unique_identifier\" ] = JSONHelper.escape_json_value( article_unique_identifier )\n",
    "        article_json[ \"title\" ] = JSONHelper.escape_json_value( article_headline )\n",
    "        article_json[ \"pub_date\" ] = JSONHelper.escape_json_value( str( article_pub_date ) )\n",
    "        article_json[ \"pdf_file_name\"] = JSONHelper.escape_json_value( article_pdf_file_name )\n",
    "        article_json[ \"text_file_name\"] = JSONHelper.escape_json_value( article_text_file_name )\n",
    "\n",
    "        # add article JSON to list\n",
    "        article_json_root.append( article_json )\n",
    "\n",
    "        # add line to PDF script\n",
    "        pdf_script_file.write( 'cp {}/{} {}/{}\\n'.format( pdf_source_directory_path,\n",
    "                                                          article_file_path,\n",
    "                                                          pdf_output_directory_path,\n",
    "                                                          article_pdf_file_name ) )\n",
    "\n",
    "        # retrieve Article_Text\n",
    "        article_text = article.article_text_set.get()\n",
    "        article_text_string = article_text.content\n",
    "\n",
    "        # got anything (we'd better...)?\n",
    "        if ( ( article_text_string is not None ) and ( article_text_string != \"\" ) ):\n",
    "        \n",
    "            # open file and output text.\n",
    "            text_file_path = \"{}/{}\".format( text_file_output_folder_path, article_text_file_name )\n",
    "            with open( text_file_path, \"w\", encoding = \"utf-8\" ) as text_file:\n",
    "\n",
    "                # output the whole thing to the file.\n",
    "                text_file.write( article_text_string )\n",
    "                \n",
    "            #-- END with open( text_file_path, \"w\", encoding = \"utf-8\" ) as text_file: --#\n",
    "\n",
    "        #-- END check to see if text to write. --#\n",
    "        \n",
    "    #-- END loop over articles. --#\n",
    "    \n",
    "#-- END with open( pdf_script_file_path, \"w\", encoding = \"utf-8\" ) as pdf_script_file: --#\n",
    "\n",
    "# output JSON file path to data folder.\n",
    "json_file_path = \"{}/publications.json\".format( data_output_directory )\n",
    "with open( json_file_path, \"w\", encoding = \"utf-8\" ) as json_file:\n",
    "\n",
    "    json.dump( article_json_root, json_file, indent = 4 )\n",
    "\n",
    "#-- END with open( json_file_path, \"w\", encoding = \"utf-8\" ) as json_file: --#\n",
    "\n",
    "print( \"Processed {} articles.\".format( article_counter ) )\n",
    "print( \"Next step, in command shell:\" )\n",
    "print( \"- make the PDF file script executable: chmod 700 {}\".format( pdf_script_file_path ) )\n",
    "print( \"- run the PDF file script: ./{}\".format( pdf_script_file_path ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output DataSet data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Make a file of DataSet data - could make CSV or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from python_utilities.json.json_helper import JSONHelper\n",
    "from context_data.models import DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data sets\n",
    "DataSet.objects.all().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all cited data sets\n",
    "DataSet.objects.filter( datasetcitation__id__gte = 0 ).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all coded citations\n",
    "DataSet.objects.filter( datasetcitation__datasetcitationdata__id__gte = 0 ).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "data_set_qs = None\n",
    "data_set_count = -1\n",
    "data_set_counter = -1\n",
    "data_set = None\n",
    "data_set_json_root = []\n",
    "data_set_json = {}\n",
    "\n",
    "# declare variables - data_set information.\n",
    "data_set_id = None\n",
    "data_set_unique_identifier = None\n",
    "data_set_title = None\n",
    "data_set_name = None\n",
    "data_set_description = None\n",
    "data_set_date = None\n",
    "data_set_coverages = None\n",
    "data_set_subjects = None\n",
    "data_set_methodology = None\n",
    "data_set_citation = None\n",
    "data_set_additional_keywords = None\n",
    "data_set_family_identifier = None\n",
    "\n",
    "# data set identifiers\n",
    "data_set_identifier_qs = None\n",
    "data_set_identifier = None\n",
    "identifier_name = None\n",
    "identifier_value = None\n",
    "identifier_dict = None\n",
    "escaped_identifier_list = None\n",
    "\n",
    "# declare variables - mentions\n",
    "mention_list = None\n",
    "escaped_mention_list = None\n",
    "mention = None\n",
    "escaped_mention = None\n",
    "\n",
    "# declare variables - output JSON file.\n",
    "json_file_path = None\n",
    "json_file = None\n",
    "\n",
    "# get only DataSets who have an associated DataSetCitationData\n",
    "data_set_qs = DataSet.objects.filter( datasetcitation__datasetcitationdata__id__gte = 0 ).distinct().order_by( \"id\" )\n",
    "data_set_count = data_set_qs.count()\n",
    "print( \"data sets to process: {}\".format( data_set_count ) )\n",
    "\n",
    "# loop over articles\n",
    "data_set_counter = 0\n",
    "for data_set in data_set_qs:\n",
    "\n",
    "    # increment counter\n",
    "    data_set_counter += 1\n",
    "\n",
    "    # get dataset information we'll use.\n",
    "    data_set_id = data_set.id\n",
    "    data_set_unique_identifier = data_set.unique_identifier\n",
    "    data_set_title = data_set.title\n",
    "    data_set_name = data_set.name\n",
    "    data_set_description = data_set.description\n",
    "    data_set_date = data_set.date\n",
    "    data_set_coverages = data_set.coverages\n",
    "    data_set_subjects = data_set.subjects\n",
    "    data_set_methodology = data_set.methodology\n",
    "    data_set_citation = data_set.citation\n",
    "    data_set_additional_keywords = data_set.additional_keywords\n",
    "    data_set_family_identifier = data_set.family_identifier\n",
    "\n",
    "    # process mentions\n",
    "    mention_list = data_set.get_unique_mention_string_list( replace_white_space_IN = True )\n",
    "\n",
    "    # build list of mentions that have been JSON-escaped.\n",
    "    escaped_mention_list = []\n",
    "    for mention in mention_list:\n",
    "        \n",
    "        # escape mention string.\n",
    "        escaped_mention = JSONHelper.escape_json_value( mention )\n",
    "            \n",
    "        # add it to the escaped list.\n",
    "        escaped_mention_list.append( escaped_mention )\n",
    "        \n",
    "    #-- END loop over mentions --#\n",
    "\n",
    "    # data set identifiers\n",
    "    data_set_identifier_qs = data_set.datasetidentifier_set.all()\n",
    "    escaped_identifier_list = []\n",
    "    for data_set_identifier in data_set_identifier_qs:\n",
    "    \n",
    "        # get values\n",
    "        identifier_name = data_set_identifier.name\n",
    "        identifier_value = data_set_identifier.identifier\n",
    "        \n",
    "        # store in dict\n",
    "        identifier_dict = {}\n",
    "        identifier_dict[ \"name\" ] = JSONHelper.escape_json_value( identifier_name )\n",
    "        identifier_dict[ \"identifier\" ] = JSONHelper.escape_json_value( identifier_value )\n",
    "        \n",
    "        # add to identifier list\n",
    "        escaped_identifier_list.append( identifier_dict )\n",
    "        \n",
    "    #-- END loop over identifiers. --#\n",
    "\n",
    "    # build data set JSON dictionary.\n",
    "    data_set_json = {}\n",
    "    data_set_json[ \"id\" ] = data_set_id\n",
    "    data_set_json[ \"unique_identifier\" ] = JSONHelper.escape_json_value( data_set_unique_identifier )\n",
    "    data_set_json[ \"title\" ] = JSONHelper.escape_json_value( data_set_title )\n",
    "    data_set_json[ \"name\" ] = JSONHelper.escape_json_value( data_set_name )\n",
    "    data_set_json[ \"description\" ] = JSONHelper.escape_json_value( data_set_description )\n",
    "    data_set_json[ \"date\" ] = JSONHelper.escape_json_value( str( data_set_date ) )\n",
    "    data_set_json[ \"coverages\" ] = JSONHelper.escape_json_value( data_set_coverages )\n",
    "    data_set_json[ \"subjects\" ] = JSONHelper.escape_json_value( data_set_subjects )\n",
    "    data_set_json[ \"methodology\" ] = JSONHelper.escape_json_value( data_set_methodology )\n",
    "    data_set_json[ \"citation\" ] = JSONHelper.escape_json_value( data_set_citation )\n",
    "    data_set_json[ \"additional_keywords\" ] = JSONHelper.escape_json_value( data_set_additional_keywords )\n",
    "    data_set_json[ \"family_identifier\" ] = JSONHelper.escape_json_value( data_set_family_identifier )\n",
    "    data_set_json[ \"mention_list\" ] = escaped_mention_list\n",
    "    data_set_json[ \"identifier_list\" ] = escaped_identifier_list\n",
    "\n",
    "    # add data set JSON to list\n",
    "    data_set_json_root.append( data_set_json )\n",
    "\n",
    "#-- END loop over datasets. --#\n",
    "\n",
    "# output JSON file path to data folder.\n",
    "json_file_path = \"{}/data_sets.json\".format( data_output_directory )\n",
    "with open( json_file_path, \"w\", encoding = \"utf-8\" ) as json_file:\n",
    "\n",
    "    json.dump( data_set_json_root, json_file, indent = 4 )\n",
    "\n",
    "#-- END with open( json_file_path, \"w\", encoding = \"utf-8\" ) as json_file: --#\n",
    "\n",
    "print( \"Processed {} data sets.\".format( data_set_counter ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output DataSetCitation data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Make a file of DataSetCitation data - data set ID, article ID, list of mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from python_utilities.json.json_helper import JSONHelper\n",
    "from context_data.models import DataSetCitation\n",
    "from context_data.models import DataSetCitationData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data sets citation\n",
    "DataSetCitation.objects.all().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all coded citations\n",
    "DataSetCitation.objects.filter( datasetcitationdata__id__gte = 0 ).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "data_set_citation_qs = None\n",
    "data_set_citation_count = -1\n",
    "data_set_citation_counter = -1\n",
    "data_set_citation = None\n",
    "data_set_citation_json_root = []\n",
    "data_set_citation_json = {}\n",
    "\n",
    "# declare variables - data_set information.\n",
    "data_set_citation_id = None\n",
    "data_set_citation_article_id = None\n",
    "data_set_citation_data_set_id = None\n",
    "\n",
    "# declare variables - mentions\n",
    "mention_list = None\n",
    "escaped_mention_list = None\n",
    "mention = None\n",
    "escaped_mention = None\n",
    "\n",
    "# declare variables - output JSON file.\n",
    "json_file_path = None\n",
    "json_file = None\n",
    "\n",
    "# get only DataSetCitations that have an associated DataSetCitationData\n",
    "data_set_citation_qs = DataSetCitation.objects.filter( datasetcitationdata__id__gte = 0 ).distinct().order_by( \"id\" )\n",
    "data_set_citation_count = data_set_citation_qs.count()\n",
    "print( \"data set citations to process: {}\".format( data_set_citation_count ) )\n",
    "\n",
    "# loop over articles\n",
    "data_set_citation_counter = 0\n",
    "for data_set_citation in data_set_citation_qs:\n",
    "\n",
    "    # increment counter\n",
    "    data_set_citation_counter += 1\n",
    "\n",
    "    # get dataset information we'll use.\n",
    "    data_set_citation_id = data_set_citation.id\n",
    "    data_set_citation_article_id = data_set_citation.article.id\n",
    "    data_set_citation_data_set_id = data_set_citation.data_set.id\n",
    "\n",
    "    # process mentions\n",
    "    mention_list = data_set_citation.get_unique_mention_string_list( replace_white_space_IN = True )\n",
    "\n",
    "    # build list of mentions that have been JSON-escaped.\n",
    "    escaped_mention_list = []\n",
    "    for mention in mention_list:\n",
    "        \n",
    "        # escape mention string.\n",
    "        escaped_mention = JSONHelper.escape_json_value( mention )\n",
    "            \n",
    "        # add it to the escaped list.\n",
    "        escaped_mention_list.append( escaped_mention )\n",
    "        \n",
    "    #-- END loop over mentions --#\n",
    "    \n",
    "    # build data set JSON dictionary.\n",
    "    data_set_citation_json = {}\n",
    "    data_set_citation_json[ \"id\" ] = data_set_citation_id\n",
    "    data_set_citation_json[ \"publication_id\" ] = data_set_citation_article_id\n",
    "    data_set_citation_json[ \"data_set_id\" ] = data_set_citation_data_set_id\n",
    "    data_set_citation_json[ \"mention_list\" ] = escaped_mention_list\n",
    "\n",
    "    # add data set JSON to list\n",
    "    data_set_citation_json_root.append( data_set_citation_json )\n",
    "\n",
    "#-- END loop over datasets. --#\n",
    "\n",
    "# output JSON file path to data folder.\n",
    "json_file_path = \"{}/data_set_citations.json\".format( data_output_directory )\n",
    "with open( json_file_path, \"w\", encoding = \"utf-8\" ) as json_file:\n",
    "\n",
    "    json.dump( data_set_citation_json_root, json_file, indent = 4 )\n",
    "\n",
    "#-- END with open( json_file_path, \"w\", encoding = \"utf-8\" ) as json_file: --#\n",
    "\n",
    "print( \"Processed {} data set citations.\".format( data_set_citation_counter ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rich_context (Python 3)",
   "language": "python",
   "name": "rich_context"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
